{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creating a Convnet",
   "id": "8585719db06601bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "#### Classifying 10 different everyday objects. The dataset i will use is built into tensorflow and called the CIFAR IMAGE Dataset. It contains 60,000 32x32 (blurs) colour images with 6000 images of each class. "
   ],
   "id": "9b9608ad0201ff02"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load and split dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "#Normalise pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'] # labels"
   ],
   "id": "42782f4630a872d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# look at one image\n",
    "IMG_INDEX = 8  # changeable\n",
    "plt.imshow(train_images[IMG_INDEX], cmap=plt.cm.binary)\n",
    "plt.xlabel(class_names[train_labels[IMG_INDEX][0]])\n",
    "plt.show()"
   ],
   "id": "d8e21b6d2a17a3c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CNN Architecture\n",
    "### A common architecture for a CNN is a stack of Conv2D and MaxPooling2D layers followed by a few densely connected layers. To idea is that the stack of convolutional and maxpooling layers extract the features from the image.\n",
    "### Layer1: the input shape of our data will be 32, 32, 3 and will process 32 filters of size 3x3 over our input data.\n",
    "### Layer 2: will perform the max pooling operation using 2x2 samples and a stride of 2.\n",
    "### Other Layers: The next set of layers do very similar things but take as input the feature map from the previous layer. Increase the frequency of filters from 32 to 64."
   ],
   "id": "2d3c1f7af6c6b14a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# building the Convolutional Base\n",
    "model = models.Sequential()\n",
    "# Layer 1\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "\n",
    "# Layer 2\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Other Layers\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ],
   "id": "cd4b7ac79ffb26d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Completed the convolutional base!! but let's have a look at these features ;)\n",
    "model.summary() # have a look at our model so far"
   ],
   "id": "11a72f5b47ce0950",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****we got 2 pixels less because the amount samples we can take. This doesn't mean much to us, just tells us about the presence of specific features as we've gone through this convolution base, which is what this is called the stack of convolution and Max pooling layers. So what we actually need to do is now pass this information into some kind of dense layer classifier, which is actually going to take this pixel data that we calculated and found, so the almost extraction of features that exist in the image, and tell us which combination of these features map to what one of these 10 classes are.****",
   "id": "3eb55da2a12dbca0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Adding Dense Layers",
   "id": "6b6985a9775e7975"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.add(layers.Flatten()) # one dimentional\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10)) # output layer"
   ],
   "id": "bafeeb9a749a5cb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.summary()",
   "id": "8deb301b5fc242df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**we can see that the flatten layer changes the shape of our data so that we can feed it to the 64 node dense layer, followed by the final output layer of 10 neurons (one of each class).**",
   "id": "211af3051af14a30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "5e1fb404a7bc58a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "# should get at least a 70% accuracy, epochs >=4 is better\n",
    "history = model.fit(train_images, train_labels, epochs=4, \n",
    "                    validation_data=(test_images, test_labels))"
   ],
   "id": "5faa55b5a6ef49dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluating the Model\n",
   "id": "af50d1b83835ccd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('Test accuracy:', test_acc)"
   ],
   "id": "b1e23883df2b2d2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Working with Small Datasets",
   "id": "9bb906dab2a5bc32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Augmentation",
   "id": "708a2ad1ba8d3dd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# creates a data generator object that transforms images\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# pick an image to transform\n",
    "test_ima  = train_images[20]\n",
    "img = image.img_to_array(test_ima) # convert image to numpy array\n",
    "img = img.reshape((1,) + img.shape) # reshape image\n",
    "\n",
    "i = 0\n",
    "\n",
    "# this loop runs forever until we break, saving images to current directory with specified prefix\n",
    "for batch in datagen.flow(img, save_prefix='test', save_format='jpeg'):\n",
    "    plt.figure(i)\n",
    "    plot = plt.imshow(image.img_to_array(batch[0]))\n",
    "    i +=1\n",
    "    if i > 4: # show 4 images\n",
    "        break\n",
    "plt.show()"
   ],
   "id": "29589babb544302e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Pretrained Models\n",
    "**We know that CNN's alone (with no dense layers) don't do anything other than map the presence of features from our input. This means we can use a pretrained CNN, one trained on millions of images, as the start of our model. This will allow us to have a very good convolutional base before adding our own dense layered classifier at the end.**"
   ],
   "id": "ee5cf43a245afb27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Using a Pretrained Model",
   "id": "e301827ab9ef7592"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "keras = tf.keras"
   ],
   "id": "1f03e3ab36d909d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset\n",
    "### We will load the cat_vs_dogs datasets from the module tensorflow_datasets."
   ],
   "id": "51d25806ddfe2499"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "# split the data manually into 8% training, 10% testing, 10% validation\n",
    "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
    "    'cats_vs_dogs',\n",
    "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "    with_info=True,\n",
    "    as_supervised=True\n",
    ")\n",
    "\n",
    "# updated on june 13"
   ],
   "id": "38c9fcb26e6811e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create a function object that we can use to get labels\n",
    "get_label_name = metadata.features['label'].int2str\n",
    "\n",
    "# display 2 images from the dataset\n",
    "for image, label in raw_train.take(5):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.title(get_label_name(label))\n"
   ],
   "id": "8f263424b5be5f55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing\n",
    "### Since the size of the images are all different, we need to convert them all to the same size -- create a function."
   ],
   "id": "54db0fe7d7959b23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "IMG_SIZE = 160 # All images will be resized to 160x160\n",
    "\n",
    "def format_example(image, label):\n",
    "# returns an image that is reshaped to IMG_SIZE\n",
    "\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image/127.5) - 1\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image, label\n"
   ],
   "id": "dfbe03c811999b7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train = raw_train.map(format_example) # apply this function to all images using map()\n",
    "validation = raw_validation.map(format_example)\n",
    "test = raw_test.map(format_example)"
   ],
   "id": "eb39caa7415efe84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# have a look at our images\n",
    "for image, label in train.take(2):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.title(get_label_name(label))"
   ],
   "id": "d0d7993545773e60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# shuffle and batch the images\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "\n",
    "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "validation_batches = validation.batch(BATCH_SIZE)\n",
    "test_batches = test.batch(BATCH_SIZE)"
   ],
   "id": "d5e648e4f39593e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# let's have a look at new images!!!\n",
    "for img, label in raw_train.take(2):\n",
    "    print(\"Original image shape: \", img.shape)\n",
    "for img, label in train.take(2):\n",
    "    print(\"New image shape: \", img.shape)\n",
    "    "
   ],
   "id": "f8d6bed757c7cabc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Picking a Pretrained Model",
   "id": "3be37806faaa26dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# create the base model from the pre-trained model MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ],
   "id": "e2aebaeccf1ff000",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "base_model.summary()",
   "id": "cd6882e9bcc4b233",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for image, _ in train.batches.take(1):\n",
    "    pass\n",
    "\n",
    "feature_batch = base_model(image)\n",
    "print(feature_batch.shape)\n",
    "# result: (32, 5, 5, 1280)"
   ],
   "id": "7afdc3f5fd8446b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Freezing the Base\n",
    "### It simply means we won't make any changes to the weights of any layers that are frozen during training."
   ],
   "id": "316217f37053802f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "base_model.trainable = False",
   "id": "63e314f16ebbbc6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "base_model.summary()",
   "id": "c58a867413ec9d90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Adding our Classifier\n",
   "id": "31e8b0a4ae0fb7e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use a global average pooling layer that will average the entire 5x5 area of each 2D feature map and return to us a single 1280 element vector per filter.\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
   ],
   "id": "1c5cfa64628b74c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# add the prediction layer that will be a single dense neuron.\n",
    "prediction_layer = keras.layers.Dense(1)"
   ],
   "id": "dea69776ad0e9041",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# combine these layers together in a model\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    global_average_layer,\n",
    "    prediction_layer\n",
    "])"
   ],
   "id": "ed3bc2bd3878d88d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.summary()",
   "id": "f2270326542424f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " # Training the Model",
   "id": "9b7c7c575617bb33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Now we will train and compile the model. We will use a very small learning rate to ensure that the model does not have any major changers made to it.",
   "id": "8e034bd1a09b5bfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_learning_rate = 0.001\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate),\n",
    "    loss=tf.keras.losses\n",
    "    # we use two classes\n",
    "    .BinaryCrossentropy  (from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ],
   "id": "71dcd9e08095be8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# evaluate the model right now to see how it does before training it on our new images\n",
    "initial_epochs= 3\n",
    "validation_steps = 20\n",
    "loss0, acc0 = model.evaluate(validation_batches, steps = validation_steps)"
   ],
   "id": "2e26d542b42269d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# now train it on our images\n",
    "history = model.fit(train_batches, epochs = initial_epochs, validation_data = validation_batches)\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "print(acc)"
   ],
   "id": "dda0db489fa96d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.save(\"dogs_vs_cats.keras\") # save the model and reload it at anytime in the future\n",
    "new_model = tf.keras.models.load_model(\"dogs_vs_cats.keras\")"
   ],
   "id": "3f3594dc059b4efa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "90066959d5b43b79",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
